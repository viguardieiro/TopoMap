{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mlpack EMST from fvec file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fvec_datasets/Cancer.fvec', 'fvec_datasets/iris.fvec', 'fvec_datasets/mfeat.fvec', 'fvec_datasets/mnist.fvec', 'fvec_datasets/seeds.fvec', 'fvec_datasets/sift_learn.fvecs']\n",
      "['Cancer', 'iris', 'mfeat', 'mnist', 'seeds', 'sift_learn']\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "path = \"fvec_datasets/\"\n",
    "path_files = [path+f for f in os.listdir(path) if os.path.isfile(path+f)]\n",
    "dataset_names = [f.split(\".\")[0].split(\"/\")[1] for f in path_files]\n",
    "print(path_files)\n",
    "print(dataset_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlpack import emst\n",
    "import dataset_utils \n",
    "import time\n",
    "import pickle \n",
    "\n",
    "aux_data = []\n",
    "for i in range(4,len(path_files)):\n",
    "    if(i ==3):\n",
    "        continue\n",
    "    path = path_files[i]\n",
    "    input = dataset_utils.fvecs_read(path)\n",
    "    start = time.time()\n",
    "    dataset_name = dataset_names[i]\n",
    "    d = emst(check_input_matrices=False, copy_all_inputs=False,\n",
    "            input_= input, leaf_size=1, naive=False, verbose=False)\n",
    "    \n",
    "    end = time.time()\n",
    "    file_path = f'saved_pickle/{dataset_name}_emst.pickle'\n",
    "    print(f'time to compute EMST for {dataset_name} using emst from MLPACK:', end - start)\n",
    "    # Open the file in binary mode\n",
    "    with open(file_path, 'wb') as file:\n",
    "    # Serialize and write the variable to the file\n",
    "        pickle.dump(d, file) \n",
    "    \n",
    "    t1 = end - start\n",
    "    input = dataset_utils.fvecs_read(path)\n",
    "    start = time.time()\n",
    "    dataset_name = dataset_names[i]\n",
    "    d = emst(check_input_matrices=False, copy_all_inputs=False,\n",
    "            input_= input, leaf_size=1, naive=True, verbose=False)\n",
    "    end = time.time() \n",
    "    print(f'time to compute EMST for {dataset_name} using emst from MLPACK naive:', end - start)\n",
    "    t2 = end - start\n",
    "    aux_data.append([t1,t2])\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(aux_data, columns=['MLPACK', 'MLPACK NAIVE'])\n",
    "df.index = dataset_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit Learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to compute EMST for Cancer using emst from scikit learn: 0.0930929183959961\n",
      "time to compute EMST for iris using emst from scikit learn: 0.0\n",
      "time to compute EMST for mfeat using emst from scikit learn: 0.8788363933563232\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 26.8 GiB for an array with shape (60000, 60000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m dataset_names[i]\n\u001b[0;32m     16\u001b[0m dists \u001b[38;5;241m=\u001b[39m squareform(pdist(\u001b[38;5;28minput\u001b[39m))\n\u001b[1;32m---> 17\u001b[0m mst \u001b[38;5;241m=\u001b[39m \u001b[43mminimum_spanning_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdists\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m     19\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     20\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaved_pickle/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_sckt_mst.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m_min_spanning_tree.pyx:93\u001b[0m, in \u001b[0;36mscipy.sparse.csgraph._min_spanning_tree.minimum_spanning_tree\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\felip\\Documents\\Github\\TopoMap\\vamana\\lib\\site-packages\\scipy\\sparse\\csgraph\\_validation.py:46\u001b[0m, in \u001b[0;36mvalidate_graph\u001b[1;34m(csgraph, directed, dtype, csr_output, dense_output, copy_if_dense, copy_if_sparse, null_value_in, null_value_out, infinity_null, nan_null)\u001b[0m\n\u001b[0;32m     44\u001b[0m         csgraph[mask] \u001b[38;5;241m=\u001b[39m null_value_out\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 46\u001b[0m         csgraph \u001b[38;5;241m=\u001b[39m \u001b[43mcsgraph_from_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnull_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnull_value_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43minfinity_null\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfinity_null\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mnan_null\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnan_null\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m csgraph\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompressed-sparse graph must be 2-D\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m_tools.pyx:215\u001b[0m, in \u001b[0;36mscipy.sparse.csgraph._tools.csgraph_from_dense\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_tools.pyx:160\u001b[0m, in \u001b[0;36mscipy.sparse.csgraph._tools.csgraph_masked_from_dense\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\felip\\Documents\\Github\\TopoMap\\vamana\\lib\\site-packages\\numpy\\ma\\core.py:2323\u001b[0m, in \u001b[0;36mmasked_values\u001b[1;34m(x, value, rtol, atol, copy, shrink)\u001b[0m\n\u001b[0;32m   2321\u001b[0m xnew \u001b[38;5;241m=\u001b[39m filled(x, value)\n\u001b[0;32m   2322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(xnew\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39mfloating):\n\u001b[1;32m-> 2323\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxnew\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43matol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrtol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2325\u001b[0m     mask \u001b[38;5;241m=\u001b[39m umath\u001b[38;5;241m.\u001b[39mequal(xnew, value)\n",
      "File \u001b[1;32mc:\\Users\\felip\\Documents\\Github\\TopoMap\\vamana\\lib\\site-packages\\numpy\\core\\numeric.py:2351\u001b[0m, in \u001b[0;36misclose\u001b[1;34m(a, b, rtol, atol, equal_nan)\u001b[0m\n\u001b[0;32m   2349\u001b[0m yfin \u001b[38;5;241m=\u001b[39m isfinite(y)\n\u001b[0;32m   2350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(xfin) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(yfin):\n\u001b[1;32m-> 2351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwithin_tol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2352\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2353\u001b[0m     finite \u001b[38;5;241m=\u001b[39m xfin \u001b[38;5;241m&\u001b[39m yfin\n",
      "File \u001b[1;32mc:\\Users\\felip\\Documents\\Github\\TopoMap\\vamana\\lib\\site-packages\\numpy\\core\\numeric.py:2332\u001b[0m, in \u001b[0;36misclose.<locals>.within_tol\u001b[1;34m(x, y, atol, rtol)\u001b[0m\n\u001b[0;32m   2330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwithin_tol\u001b[39m(x, y, atol, rtol):\n\u001b[0;32m   2331\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m errstate(invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m), _no_nep50_warning():\n\u001b[1;32m-> 2332\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m less_equal(\u001b[38;5;28mabs\u001b[39m(\u001b[43mx\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43my\u001b[49m), atol \u001b[38;5;241m+\u001b[39m rtol \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mabs\u001b[39m(y))\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 26.8 GiB for an array with shape (60000, 60000) and data type float64"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.sparse.csgraph import minimum_spanning_tree\n",
    "\n",
    "\n",
    "\n",
    "import dataset_utils \n",
    "import time\n",
    "import pickle \n",
    "\n",
    "aux_data = []\n",
    "for i in range(len(path_files)):\n",
    "    path = path_files[i]\n",
    "    input = dataset_utils.fvecs_read(path)\n",
    "    start = time.time()\n",
    "    dataset_name = dataset_names[i]\n",
    "    dists = squareform(pdist(input))\n",
    "    mst = minimum_spanning_tree(dists).toarray()\n",
    "   \n",
    "    end = time.time()\n",
    "    file_path = f'saved_pickle/{dataset_name}_sckt_mst.pickle'\n",
    "    print(f'time to compute EMST for {dataset_name} using emst from scikit learn:', end - start)\n",
    "    # Open the file in binary mode\n",
    "    with open(file_path, 'wb') as file:\n",
    "    # Serialize and write the variable to the file\n",
    "        pickle.dump(mst, file) \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Vamana Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From graph header, expected file size:10838324\n",
      " max_observed_degree:32 \n",
      " file_frozen_pts: 0\n",
      " \n",
      "done. Index has 100000 nodes\n",
      "--\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import dataset_utils\n",
    "from dataset_utils import Index_read, Compute_adj_matrix\n",
    "path = \"C:/Users/felip/Documents/Github/DiskANN/data/sift/index_sift_learn_R32_L50_A1.2\"\n",
    "adj_dict = dataset_utils.Index_read(path)\n",
    "sparse_matrix = dataset_utils.Compute_adj_matrix(adj_dict,\"fvec_datasets/sift_learn.fvecs\")\n",
    "print(sparse_matrix.count_nonzero())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': 0, 'world': 1, 'goodbye': 2, 'cruel': 3}\n",
      "[0, 3, 6]\n",
      "[0, 1, 0, 2, 3, 1]\n",
      "[1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2, 1, 0, 0],\n",
       "       [0, 1, 1, 1]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "docs = [[\"hello\", \"world\", \"hello\"], [\"goodbye\", \"cruel\", \"world\"]]\n",
    "indptr = [0]\n",
    "indices = []\n",
    "data = []\n",
    "vocabulary = {}\n",
    "for d in docs:\n",
    "    for term in d:\n",
    "        index = vocabulary.setdefault(term, len(vocabulary))\n",
    "        indices.append(index)\n",
    "        data.append(1)\n",
    "    indptr.append(len(indices))\n",
    "\n",
    "print(vocabulary)\n",
    "print(indptr)\n",
    "print(indices)\n",
    "print(data)\n",
    "csr_matrix((data, indices, indptr), dtype=int).toarray()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vamana",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
